{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "from bert_multiple_choice.data_module import DataCollatorForMultipleChoice\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForMultipleChoice, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2190b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"multiple_choice/experiments\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"multiple_choice/experiments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = \"hello , how are you doing tonight ? i am great . i just got back from the club . i would rather eat chocolate cake during this season . i went to club chino . what show are you watching ? do you live in a house or apartment ? i love those shows . i am really craving cake . it matters because i have a sweet tooth . my family lives in alaska . it is freezing down there .\"\n",
    "choice0 = \"i just bought a brand new house. i like to dance at the club. i run a dog obedience school. i have a big sweet tooth. i like taking and posting selkies.\"\n",
    "choice1 = \"i love to meet new people. i have a turtle named timothy. my favorite sport is ultimate frisbee. my parents are living in bora bora. autumn is my favorite season.\"\n",
    "choice2 = \"i just bought a brand new house. i like to dance at the club. i run a dog obedience school. i like taking and posting selkies.\"\n",
    "\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "encoding = tokenizer([history, history, history], [choice2, choice1, choice2], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=None)  # batch size is 1\n",
    "\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "logits = logits.detach().cpu().numpy()\n",
    "\n",
    "print(np.argmax(logits, axis=-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona_env",
   "language": "python",
   "name": "persona_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
