{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3ba4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "from bert_multiple_choice.data_module import DataCollatorForMultipleChoice\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForMultipleChoice, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2190b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file multiple_choice/experiments/added_tokens.json. We won't load it.\n",
      "loading file multiple_choice/experiments/vocab.txt\n",
      "loading file multiple_choice/experiments/tokenizer.json\n",
      "loading file None\n",
      "loading file multiple_choice/experiments/special_tokens_map.json\n",
      "loading file multiple_choice/experiments/tokenizer_config.json\n",
      "loading configuration file multiple_choice/experiments/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google/bert_uncased_L-2_H-128_A-2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMultipleChoice\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"persona\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file multiple_choice/experiments/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMultipleChoice.\n",
      "\n",
      "All the weights of BertForMultipleChoice were initialized from the model checkpoint at multiple_choice/experiments.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMultipleChoice for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"multiple_choice/experiments\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"multiple_choice/experiments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eca6e43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "history = \"hello , how are you doing tonight ? i am great . i just got back from the club . i would rather eat chocolate cake during this season . i went to club chino . what show are you watching ? do you live in a house or apartment ? i love those shows . i am really craving cake . it matters because i have a sweet tooth . my family lives in alaska . it is freezing down there .\"\n",
    "choice0 = \"i just bought a brand new house. i like to dance at the club. i run a dog obedience school. i have a big sweet tooth. i like taking and posting selkies.\"\n",
    "choice1 = \"i love to meet new people. i have a turtle named timothy. my favorite sport is ultimate frisbee. my parents are living in bora bora. autumn is my favorite season.\"\n",
    "choice2 = \"i just bought a brand new house. i like to dance at the club. i run a dog obedience school. i like taking and posting selkies.\"\n",
    "\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "encoding = tokenizer([history, history, history], [choice2, choice1, choice2], return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=None)  # batch size is 1\n",
    "\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "logits = logits.detach().cpu().numpy()\n",
    "\n",
    "print(np.argmax(logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15c9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona_env",
   "language": "python",
   "name": "persona_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
